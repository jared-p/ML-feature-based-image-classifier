{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "845c076f",
   "metadata": {},
   "source": [
    "# ENGR418 Project Stage 2 Group 31\n",
    "\n",
    "By: Jared Paull (63586572), Liam Ross (75469692)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "9f1f3355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from PIL import Image, ImageFilter\n",
    "import PIL\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2b4dd8",
   "metadata": {},
   "source": [
    "## Single Function Call\n",
    "\n",
    "The function in the cell below can be called to run the entire algorithm. Before running, be sure to run the cells containing the functions at the bottom of this page, or else errors will be thrown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "69ce5bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Predicted  Circle  Rectangle  Square\n",
      "Shape Actual                              \n",
      "Circle               27          0       0\n",
      "Rectangle             0         27       0\n",
      "Square                0          0      27\n",
      "Percentage of correct classification from model on training data set: 100.00%\n",
      "\n",
      "Shape Predicted  Circle  Rectangle  Square\n",
      "Shape Actual                              \n",
      "Circle               24          0       3\n",
      "Rectangle             0         27       0\n",
      "Square                0          0      27\n",
      "Percentage of correct classification from model on testing data set: 96.30%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from PIL import Image, ImageFilter\n",
    "import PIL\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn\n",
    "\n",
    "# first param is the relative training data directory\n",
    "# second param is the relative testing data directory\n",
    "\n",
    "training_data_relative_dir = \"../data/training\"\n",
    "testing_data_relative_dir = \"../data/testing\"\n",
    "\n",
    "# this will take a 1-2 minutes to run (depending on device capabilities)\n",
    "test_function(training_data_relative_dir, testing_data_relative_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a2c803",
   "metadata": {},
   "source": [
    "## Setting Tuning Parameters\n",
    "\n",
    "pefore starting, these parameters must be set, they can be tuned to optimize performance though. The optimal values found are considered as default below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "757c3f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_size=64, filter_value=4, angles=0->180 increments by 2, 100% & 96.30%\n",
    "\n",
    "# image size will dictate the size each image will be reshapet to later, used for tuning\n",
    "image_size = 64\n",
    "# filter value sets a threshold value on the edge detection image, used for tuning\n",
    "filter_value = 4\n",
    "# list of angles that the algorithm will rotate though, used for tuning\n",
    "angles = []\n",
    "for i in range(90):\n",
    "    angles.append(2*i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48204daa",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "Next, all image data is scrapped from the image files in their respective relative directory. Refer to get_image_feature_data function for line by line description. In essense, each image will have 4 feature vectors. One for maximum Lego brick length, one for minimum lego brick length, and one for both average and median Lego brick length. These values will come from the rotated image, which is further discussed in their own respective functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "a53f9cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets all training data from relative directory.\n",
    "# refer to functions at bottom for line-by line commenting\n",
    "x,y = get_image_feature_data(\"../data/training\", image_size, filter_value, angles)\n",
    "# gets all testing data from relative directory.\n",
    "xt, yt = get_image_feature_data(\"../data/testing\", image_size, filter_value, angles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bece38",
   "metadata": {},
   "source": [
    "### Classifier Training\n",
    "\n",
    "Now with the feature vectors selected and image data collected, a classifier can be trained. After testing, we opted to use a k neighbors classifier which provided the best results. The classifier will classify a point based on the classification of points near it, making it simple and quick to calcualte. With strong engineered features, the k neighbors classifier provides a high accuracy solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "c6026e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a non linear k-nearest neighbor classifier that considers the 8th nearest neighbors where each neighbor is weighted by distance\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "k_neighbors = KNeighborsClassifier(n_neighbors=8, weights=\"distance\")\n",
    "\n",
    "# fits data to the classifier\n",
    "k_neighbors.fit(x,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071b3fb2",
   "metadata": {},
   "source": [
    "## **Prediction, Confusion Matrices, and Accuracy of Classifier**\n",
    "\n",
    "### Classifier Prediction: Training Data\n",
    "\n",
    "Now, the classifier can be tested. First it is tested with the training image data. The results of the confusion matrix, as well as the accuracy of the algorithm are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "ad369fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Predicted  Circle  Rectangle  Square\n",
      "Shape Actual                              \n",
      "Circle               27          0       0\n",
      "Rectangle             0         27       0\n",
      "Square                0          0      27\n",
      "Percentage of correct classification from model on training data set: 100.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# feeds the training data back into the classifier for predicition\n",
    "pred =  k_neighbors.predict(x)\n",
    "# formats the prediction values to string labels (refer to function below)\n",
    "predicted = confusion_format(pred)\n",
    "# foramts the actual labels to string labels (refer to function below)\n",
    "actual = confusion_format(y)\n",
    "# prints the confusion matrix using string labels\n",
    "print(pd.crosstab(actual, predicted, rownames=[\"Shape Actual\"], colnames=[\"Shape Predicted\"]))\n",
    "# prints the error percentage (refer to function below)\n",
    "print(f\"Percentage of correct classification from model on training data set: {100-error_percentage(pred,y):.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8851b7",
   "metadata": {},
   "source": [
    "### Classifier Prediction: Testing Data\n",
    "\n",
    "Next it is tested with the testing image data. The testing image is a better recognition of the classifiers accuracy since it is being fed images that it has never seen before. The results of the confusion matrix, as well as the accuracy of the algorithm are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "5505079b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Predicted  Circle  Rectangle  Square\n",
      "Shape Actual                              \n",
      "Circle               24          0       3\n",
      "Rectangle             0         27       0\n",
      "Square                0          0      27\n",
      "Percentage of correct classification from model on testing data set: 96.30%\n"
     ]
    }
   ],
   "source": [
    "# feeds the testing data into the classifier for prediction\n",
    "pred =  k_neighbors.predict(xt)\n",
    "# formats the prediction values to string labels (refer to function below)\n",
    "predicted = confusion_format(pred)\n",
    "# formats teh actual labels to string values (refer to function below)\n",
    "actual = confusion_format(yt)\n",
    "# prints the confusion matrix using string labels\n",
    "print(pd.crosstab(actual, predicted, rownames=[\"Shape Actual\"], colnames=[\"Shape Predicted\"]))\n",
    "# prints the error percentage (refer to function below)\n",
    "print(f\"Percentage of correct classification from model on testing data set: {100-error_percentage(pred,y):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5d1334",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eed9fa",
   "metadata": {},
   "source": [
    "# **Functions**\n",
    "\n",
    "All of these functions **must** be ran before anything else. Each function has its purpose discussed, and are each well commented on.\n",
    "\n",
    "\n",
    "### edge_image\n",
    "\n",
    "This function takes in a raw Lego brick image, then exports a filtered, binary, edge detected version. This means, it will output an image that only contains 0/1 in monochrome. All 1's will dictate edges of the Lego brick, while 0's indicate blank space that is not useful. All noise outside of the lego bricks edge should be filtered to allow the get_len to get the correct brick length. Noise going into the get_len function will make the classifier unrealiable. The function is commented on in detail below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa908503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns image that is the filtered, reshaped, edge detection version\n",
    "def edge_image(image, image_size, filter_value):\n",
    "    # takes input image and converts to monochrome\n",
    "    image = image.convert(\"L\")\n",
    "    # converts the monochrome image to an edge detection version (note this image is ripe with noise)\n",
    "    image = image.filter(ImageFilter.FIND_EDGES)\n",
    "    # Compress image down to 18x18 image, will blur specific noise in the image to make lego brick obvious\n",
    "    image = image.resize((16 + 2,16 + 2))\n",
    "    # simply slices off the outter pixel of the image, border/edge pixels are recognized as ...\n",
    "    # a \"change\" in colour, thus are labeled as an edge, the next line will slice out this edge error.\n",
    "    # will output a 16x16 image\n",
    "    image = PIL.Image.fromarray(np.array(image)[int(1) : int(image.height -1), int(1) : int(image.width - 1)])\n",
    "    # resizes image from 16x16 to desizered image size, return information to the plot.\n",
    "    # resizing down then back up was to blur out and specific noise, so all noise can be easily filtered later.\n",
    "    image = image.resize((image_size,image_size))\n",
    "    \n",
    "    # converts the image to a numpy array\n",
    "    data = np.asarray(image)\n",
    "    # filters out any noise in the image\n",
    "    data[data <= filter_value] = 0\n",
    "    # converts image from monochrome values to binary (for ease of interpretation)\n",
    "    data[data > 0] = 1\n",
    "    \n",
    "    # converts the image data back to a Pillow image object for further use\n",
    "    image = PIL.Image.fromarray(data)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab808f8",
   "metadata": {},
   "source": [
    "### get_len\n",
    "\n",
    "used to get the length between the top-most pixel, and the bottom-most pixel in the image. Takes image from edge_image function and will return a single integer representing the lenght described above by rotating slowly, and taking length at each step. We can piece together what the lego brick is by examining the values it takes as it rotates. Function takes image from edge_image function above, thus requires very little background noise to work effectively.\n",
    "\n",
    "Expect circles to remain similar in value as it rotates. Expect rectangles to have a large maximum value. Expect circles to have a maximum value greater than circle but less then rectangle. Will use max/min/avg/med later to examine the changes over angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "b9f4dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_len(image):\n",
    "    # converts image to numpy array\n",
    "    data = np.array(image)\n",
    "    # represents the lowest pixel index (index represents height where bottom of image is zero)\n",
    "    # initialize quantity to top of image to guarantee it will decrease (assuming image has a non-zero pixel)\n",
    "    min_index = image.height\n",
    "    # represents the highest pixel index (index represents height where top of image is maximum value, i.e. image height)\n",
    "    # initialize quantity to bottom of image to guarantee it will increase (assuming image has a non-zero pixel)\n",
    "    max_index = 0\n",
    "    \n",
    "    # first loop starts from bottom of image and will crawl upwards\n",
    "    for i in range(image.height):\n",
    "        # nested loop will examing each pixel from left to right by height\n",
    "        for j in range(image.width):\n",
    "            # if a edge is detected (lego brick is found)\n",
    "            if( data[i][j] == 1):\n",
    "                # sets min index if current height index is less then smallest index found far\n",
    "                if (min_index > i):\n",
    "                    min_index = i\n",
    "                # sets max index if current height index is greater than greatest index found thus far\n",
    "                if( max_index < i):\n",
    "                    max_index = i\n",
    "    # finally, return difference between max height and min height to get vertical length the image takes up\n",
    "    return max_index - min_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b67131d",
   "metadata": {},
   "source": [
    "### get_image_feature_data\n",
    "\n",
    "Will iterate through a directory and gather feature data from each image, as well as its corresponding correct label. Is largely an accumulation of edge_image and get_len functions that is iterated for each image. Will return *x*, and *y* which are the feature vectors, and feature labels respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a512ff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_feature_data(rel_dir, image_size, filter_value, angles):\n",
    "    # initializes feature data and labels for use population later\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    # will loop through each file in rel_dir directory\n",
    "    for pic in os.listdir(rel_dir):\n",
    "        # creates new Pillow image object from pic in relative directory\n",
    "        image = PIL.Image.open(f\"{rel_dir}/{pic}\")\n",
    "        # calls function to get filtered, reshaped, edge detection version of image.\n",
    "        image = edge_image(image, image_size, filter_value)\n",
    "        \n",
    "        # initialize list to propogate with lengths for different angles\n",
    "        vec = []\n",
    "    \n",
    "        # for each loop to rotate through all angles the algorithm considers\n",
    "        for angle in angles:\n",
    "            # rotates original image by angle in for each loop\n",
    "            img = image.rotate(angle)\n",
    "            # for specific angle, find the length using the function described above\n",
    "            length = get_len(img)\n",
    "            # append new length to list containing length for each angle\n",
    "            vec.append(length)\n",
    "        # converts list to array to make math more efficient\n",
    "        vec = np.array(vec)\n",
    "        # maximum length recorded between all angles, normalized by height\n",
    "        # useful for identifying rectangles\n",
    "        max_len = np.max(vec) / img.height\n",
    "        # minimum length recorded between all angles, normalized by height\n",
    "        min_len = np.min(vec) / img.height\n",
    "        # average length recorded between all angles, normalized by height\n",
    "        # useful for identifying circles\n",
    "        avg_len = np.average(vec) / img.height\n",
    "        # median length recorded between all angles, normalized by height\n",
    "        # useful for identifying circles\n",
    "        med_len = np.median(vec) / img.height\n",
    "        # dynamically override vec to be list of 4 key values from list of lengths by angle\n",
    "        vec = [max_len, min_len, avg_len, med_len]\n",
    "\n",
    "        # examine the name of the picture file, can find correct label based on first letter of the file name.\n",
    "        # c indicates the picture is a circle\n",
    "        if( str.lower(pic[0]) == \"c\"):\n",
    "            # classify circles as a 0\n",
    "            y.append(0)\n",
    "        # r indicates the picture is a rectangle\n",
    "        elif (str.lower(pic[0]) == \"r\"):\n",
    "            # classify rectangle as a 1\n",
    "            y.append(1)\n",
    "        # only other situation is the image is a square\n",
    "        else:\n",
    "            # classify square as a 2\n",
    "            y.append(2)\n",
    "\n",
    "        x.append(vec) # each image has 1536 features\n",
    "\n",
    "    # convert feature vector data/labels from lists to arrays for ease of use in the classifier model\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "fc53cda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will convert from decimal label to strings. Very easy to comprehend\n",
    "# 0=>Circle, 1=>Rectangle, 2=>Square\n",
    "def confusion_format(labels):\n",
    "    test = []\n",
    "    for i in labels:\n",
    "        if i == 0:\n",
    "            test.append(\"Circle\")\n",
    "        elif i == 1:\n",
    "            test.append(\"Rectangle\")\n",
    "        else:\n",
    "            test.append(\"Square\")\n",
    "    test = np.array(test)\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "467dbde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to calculate error percentage by looking at number of differences between prediction and actual labels.\n",
    "def error_percentage(pred, y):\n",
    "    # the number of errors is the number of differences between the model's labels and the correct labels\n",
    "    errors = 0\n",
    "    for i in range(pred.size):\n",
    "        # pred is the predicted array labels, while y is the actual\n",
    "        if pred[i] != y[i]:\n",
    "            errors = errors + 1\n",
    "            \n",
    "    # then the percentage of errors is the number of errors divided by the total number of image samples times 100 for percentage.\n",
    "    return errors / pred.size * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "d739f1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function is exclusively to demonstrate all code in a single function call. Refer to cells above, or functions described for ...\n",
    "# in depth description/line-by-line description.\n",
    "def test_function(training_dir, testing_dir):\n",
    "    image_size = 64\n",
    "    filter_value = 4\n",
    "    angles = []\n",
    "    for i in range(90):\n",
    "        angles.append(2*i)\n",
    "    \n",
    "    x,y = get_image_feature_data(training_dir, image_size, filter_value, angles)\n",
    "    xt, yt = get_image_feature_data(testing_dir, image_size, filter_value, angles)\n",
    "    k_neighbors = KNeighborsClassifier(8, weights=\"distance\")\n",
    "    k_neighbors.fit(x,y);\n",
    "    \n",
    "    \n",
    "    pred =  k_neighbors.predict(x)\n",
    "    predicted = confusion_format(pred)\n",
    "    actual = confusion_format(y)\n",
    "    print(pd.crosstab(actual, predicted, rownames=[\"Shape Actual\"], colnames=[\"Shape Predicted\"]))\n",
    "    print(f\"Percentage of correct classification from model on training data set: {100-error_percentage(pred,y):.2f}%\\n\")\n",
    "    \n",
    "    pred =  k_neighbors.predict(xt)\n",
    "    predicted = confusion_format(pred)\n",
    "    actual = confusion_format(yt)\n",
    "    print(pd.crosstab(actual, predicted, rownames=[\"Shape Actual\"], colnames=[\"Shape Predicted\"]))\n",
    "    print(f\"Percentage of correct classification from model on testing data set: {100-error_percentage(pred,y):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1656100",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
